{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f5ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid more than 1 element per cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42321f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "\n",
    "class WarehouseEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Environment representing a warehouse where a robot navigates to pick up packages and deliver them to designated points.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, grid_size, num_packages, num_delivery_points):\n",
    "        \"\"\"\n",
    "        Initializes the Warehouse environment.\n",
    "\n",
    "        Parameters:\n",
    "        - grid_size (int): Size of the grid layout.\n",
    "        - num_packages (int): Number of packages in the warehouse.\n",
    "        - num_delivery_points (int): Number of delivery points in the warehouse.\n",
    "        \"\"\"\n",
    "        super(WarehouseEnv, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.num_packages = num_packages\n",
    "        self.num_delivery_points = num_delivery_points\n",
    "        self.action_space = spaces.Discrete(6)  # Up, Down, Left, Right, Pick up, Drop off\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(grid_size),  # Robot X position\n",
    "            spaces.Discrete(grid_size),  # Robot Y position\n",
    "            spaces.MultiBinary(num_packages),  # Package locations\n",
    "            spaces.MultiBinary(num_delivery_points),  # Delivery locations\n",
    "            spaces.MultiBinary(num_packages)  # Inventory\n",
    "        ))\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment to its initial state.\n",
    "\n",
    "        Returns:\n",
    "        - observation (tuple): Initial observation of the environment.\n",
    "        \"\"\"\n",
    "        self.robot_pos = [0, 0]\n",
    "        self.packages = np.zeros(self.grid_size)\n",
    "        self.delivery_points = np.zeros(self.grid_size)\n",
    "        self.inventory = np.zeros(self.num_packages)\n",
    "\n",
    "        # Randomly place at least one package without overlapping with delivery points\n",
    "        placed_package = False\n",
    "        while not placed_package:\n",
    "            rand_x = np.random.randint(self.grid_size)\n",
    "            rand_y = np.random.randint(self.grid_size)\n",
    "            while self.delivery_points[rand_x] == 1:\n",
    "                rand_x = np.random.randint(self.grid_size)\n",
    "                rand_y = np.random.randint(self.grid_size)\n",
    "            self.packages[rand_x] = 1\n",
    "            placed_package = True\n",
    "\n",
    "        # Ensure delivery points are placed without overlapping with packages\n",
    "        placed_delivery = 0\n",
    "        while placed_delivery < self.num_delivery_points:\n",
    "            rand_x = np.random.randint(self.grid_size)\n",
    "            if self.delivery_points[rand_x] == 0 and self.packages[rand_x] == 0:\n",
    "                self.delivery_points[rand_x] = 1\n",
    "                placed_delivery += 1\n",
    "\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Executes one time step in the environment.\n",
    "\n",
    "        Parameters:\n",
    "        - action (int): Action to be taken by the agent.\n",
    "\n",
    "        Returns:\n",
    "        - observation (tuple): New observation of the environment.\n",
    "        - reward (float): Reward received from the environment.\n",
    "        - done (bool): Whether the episode is done or not.\n",
    "        - info (dict): Additional information about the environment.\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        done = False\n",
    "        new_pos = self.robot_pos[:]\n",
    "\n",
    "        if action == 0:  # Move Up\n",
    "            new_pos[0] -= 1\n",
    "        elif action == 1:  # Move Down\n",
    "            new_pos[0] += 1\n",
    "        elif action == 2:  # Move Left\n",
    "            new_pos[1] -= 1\n",
    "        elif action == 3:  # Move Right\n",
    "            new_pos[1] += 1\n",
    "        elif action == 4:  # Pick up\n",
    "            if tuple(self.robot_pos) in self.packages:\n",
    "                package_index = list(self.robot_pos).index(1)\n",
    "                self.inventory[package_index] = 1\n",
    "                self.packages[package_index] = 0\n",
    "                reward += 10\n",
    "            else:\n",
    "                reward -= 1\n",
    "        elif action == 5:  # Drop off\n",
    "            if tuple(self.robot_pos) in self.delivery_points:\n",
    "                if 1 in self.inventory:\n",
    "                    package_index = list(self.inventory).index(1)\n",
    "                    self.inventory[package_index] = 0\n",
    "                    reward += 100\n",
    "                    # Check if all packages have been delivered\n",
    "                    if np.sum(self.inventory) == 0:\n",
    "                        done = True  # Episode ends if all packages are delivered\n",
    "                else:\n",
    "                    reward -= 10\n",
    "            else:\n",
    "                reward -= 10\n",
    "\n",
    "        if 0 <= new_pos[0] < self.grid_size and 0 <= new_pos[1] < self.grid_size:\n",
    "            self.robot_pos = new_pos  # Update the robot's position\n",
    "\n",
    "        observation = self._get_observation()\n",
    "        return observation, reward, done, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Returns the current observation of the environment.\n",
    "\n",
    "        Returns:\n",
    "        - observation (tuple): Current observation of the environment.\n",
    "        \"\"\"\n",
    "        return tuple(self.robot_pos + [int(x) for x in self.packages] + [int(x) for x in self.delivery_points] + [int(x) for x in self.inventory])\n",
    "\n",
    "def draw_env(screen, env):\n",
    "    \"\"\"\n",
    "    Draws the current state of the environment on the screen.\n",
    "\n",
    "    Parameters:\n",
    "    - screen (pygame.Surface): Pygame surface representing the screen.\n",
    "    - env (WarehouseEnv): Instance of the Warehouse environment.\n",
    "    \"\"\"\n",
    "    screen.fill((255, 255, 255))\n",
    "    cell_size = 50\n",
    "    grid_size = env.grid_size\n",
    "    robot_img = pygame.image.load(\"robot.png\").convert_alpha()\n",
    "    package_img = pygame.image.load(\"package.png\").convert_alpha()\n",
    "    delivery_img = pygame.image.load(\"delivery.png\").convert_alpha()\n",
    "\n",
    "    for i in range(grid_size):\n",
    "        pygame.draw.line(screen, (0, 0, 0), (i * cell_size, 0), (i * cell_size, grid_size * cell_size))\n",
    "        pygame.draw.line(screen, (0, 0, 0), (0, i * cell_size), (grid_size * cell_size, i * cell_size))\n",
    "\n",
    "    robot_pos = env.robot_pos\n",
    "    screen.blit(robot_img, (robot_pos[1] * cell_size, robot_pos[0] * cell_size))\n",
    "\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            if env.packages[i] == 1:\n",
    "                screen.blit(package_img, (j * cell_size, i * cell_size))\n",
    "            if env.delivery_points[i] == 1:\n",
    "                screen.blit(delivery_img, (j * cell_size, i * cell_size))\n",
    "\n",
    "    pygame.display.flip()\n",
    "\n",
    "# Q-learning parameters\n",
    "gamma = 0.9\n",
    "alpha = 0.1\n",
    "\n",
    "def q_learning(env, num_episodes=1000, max_steps_per_episode=3):\n",
    "    \"\"\"\n",
    "    Performs Q-learning to train the agent.\n",
    "\n",
    "    Parameters:\n",
    "    - env (WarehouseEnv): Instance of the Warehouse environment.\n",
    "    - num_episodes (int): Number of episodes for training.\n",
    "    - max_steps_per_episode (int): Maximum number of steps per episode.\n",
    "    \"\"\"\n",
    "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        while not done and steps < max_steps_per_episode:\n",
    "            action = np.argmax(q_table[state])\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            q_value = q_table[state, action]\n",
    "            max_next_q_value = np.max(q_table[next_state])\n",
    "            new_q_value = q_value + alpha * (reward + gamma * max_next_q_value - q_value)\n",
    "            q_table[state, action] = new_q_value\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "        print(f\"Episode: {episode+1}, Total Reward: {total_reward}\")\n",
    "\n",
    "def main():\n",
    "    env = WarehouseEnv(grid_size=5, num_packages=3, num_delivery_points=2)\n",
    "\n",
    "    pygame.init()\n",
    "    screen = pygame.display.set_mode((env.grid_size * 50, env.grid_size * 50))\n",
    "    clock = pygame.time.Clock()\n",
    "\n",
    "    num_episodes = 1000\n",
    "    max_steps_per_episode = 100  # Define maximum steps per episode\n",
    "    for episode in range(num_episodes):\n",
    "        print(f\"Current Episode: {episode+1}\")  # Print current episode\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps = 0  # Initialize step count\n",
    "        exit_flag = False  # Flag to exit event handling loop\n",
    "        while not done and steps < max_steps_per_episode:  # Check maximum steps condition\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    exit_flag = True  # Set the exit flag to True\n",
    "                    break  # Exit the event handling loop\n",
    "            if exit_flag:\n",
    "                break  # Exit the episode loop if the exit flag is True\n",
    "            draw_env(screen, env)\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            clock.tick(10)\n",
    "            steps += 1  # Increment step count\n",
    "        print(f\"Episode: {episode+1}, Total Reward: {total_reward}\")\n",
    "\n",
    "    pygame.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c950be0f",
   "metadata": {},
   "source": [
    "# TASK IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ab650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# α=0.5, γ= 0.5 for 2 hyper parameters. In addition, about the policy, εvalue starts at 0.9,\n",
    "#and it is reduced by multiplied by 0.9999 until εvalue =0.5; then it decreases again by multiplied by 0.999 until εvalue = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bb9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "\n",
    "class WarehouseEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Environment representing a warehouse where a robot navigates to pick up packages and deliver them to designated points.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, grid_size, num_packages, num_delivery_points):\n",
    "        \"\"\"\n",
    "        Initializes the Warehouse environment.\n",
    "\n",
    "        Parameters:\n",
    "        - grid_size (int): Size of the grid layout.\n",
    "        - num_packages (int): Number of packages in the warehouse.\n",
    "        - num_delivery_points (int): Number of delivery points in the warehouse.\n",
    "        \"\"\"\n",
    "        super(WarehouseEnv, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.num_packages = num_packages\n",
    "        self.num_delivery_points = num_delivery_points\n",
    "        self.action_space = spaces.Discrete(6)  # Up, Down, Left, Right, Pick up, Drop off\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(grid_size),  # Robot X position\n",
    "            spaces.Discrete(grid_size),  # Robot Y position\n",
    "            spaces.MultiBinary(num_packages),  # Package locations\n",
    "            spaces.MultiBinary(num_delivery_points),  # Delivery locations\n",
    "            spaces.MultiBinary(num_packages)  # Inventory\n",
    "        ))\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment to its initial state.\n",
    "\n",
    "        Returns:\n",
    "        - observation (tuple): Initial observation of the environment.\n",
    "        \"\"\"\n",
    "        self.robot_pos = [0, 0]\n",
    "        self.packages = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.delivery_points = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.obstacles = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.inventory = np.zeros(self.num_packages)\n",
    "        self.package_to_delivery = {}  # Dictionary to store package to delivery point associations\n",
    "\n",
    "        # Randomly place delivery points\n",
    "        for _ in range(self.num_delivery_points):\n",
    "            rand_x = np.random.randint(self.grid_size)\n",
    "            rand_y = np.random.randint(self.grid_size)\n",
    "            while self.delivery_points[rand_x][rand_y] == 1:  # Ensure unique location for each delivery point\n",
    "                rand_x = np.random.randint(self.grid_size)\n",
    "                rand_y = np.random.randint(self.grid_size)\n",
    "            self.delivery_points[rand_x][rand_y] = 1\n",
    "\n",
    "        # Assign packages to remaining empty locations\n",
    "        placed_packages = 0\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                if placed_packages < self.num_packages and self.delivery_points[i][j] == 0:\n",
    "                    self.packages[i][j] = 1\n",
    "                    # Find the nearest delivery point and associate the package with it\n",
    "                    dist_to_delivery = np.inf\n",
    "                    nearest_delivery = None\n",
    "                    for di in range(self.grid_size):\n",
    "                        for dj in range(self.grid_size):\n",
    "                            if self.delivery_points[di][dj] == 1:\n",
    "                                dist = abs(i - di) + abs(j - dj)\n",
    "                                if dist < dist_to_delivery:\n",
    "                                    dist_to_delivery = dist\n",
    "                                    nearest_delivery = (di, dj)\n",
    "                    self.package_to_delivery[(i, j)] = nearest_delivery\n",
    "                    placed_packages += 1\n",
    "\n",
    "        # Randomly place obstacles\n",
    "        num_obstacles = 0\n",
    "        while num_obstacles < 2:  # Add 2 obstacles per episode\n",
    "            rand_x = np.random.randint(self.grid_size)\n",
    "            rand_y = np.random.randint(self.grid_size)\n",
    "            if (\n",
    "                self.packages[rand_x][rand_y] == 0\n",
    "                and self.delivery_points[rand_x][rand_y] == 0\n",
    "                and self.obstacles[rand_x][rand_y] == 0\n",
    "            ):\n",
    "                self.obstacles[rand_x][rand_y] = 1\n",
    "                num_obstacles += 1\n",
    "\n",
    "        self.q_table = np.zeros((2 ** (self.grid_size * 2 + self.num_packages * 2), self.action_space.n))\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Executes one time step in the environment.\n",
    "\n",
    "        Parameters:\n",
    "        - action (int): Action to be taken by the agent.\n",
    "\n",
    "        Returns:\n",
    "        - observation (tuple): New observation of the environment.\n",
    "        - reward (float): Reward received from the environment.\n",
    "        - done (bool): Whether the episode is done or not.\n",
    "        - info (dict): Additional information about the environment.\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        done = False\n",
    "        new_pos = self.robot_pos[:]\n",
    "\n",
    "        if action == 0:  # Move Up\n",
    "            new_pos[0] -= 1\n",
    "        elif action == 1:  # Move Down\n",
    "            new_pos[0] += 1\n",
    "        elif action == 2:  # Move Left\n",
    "            new_pos[1] -= 1\n",
    "        elif action == 3:  # Move Right\n",
    "            new_pos[1] += 1\n",
    "        elif action == 4:  # Pick up\n",
    "            if tuple(self.robot_pos) in self.packages:\n",
    "                package_index = list(self.robot_pos).index(1)\n",
    "                self.inventory[package_index] = 1\n",
    "                self.packages[self.robot_pos[0]][self.robot_pos[1]] = 0\n",
    "                reward += 10  # Add a positive reward for picking up a package\n",
    "            else:\n",
    "                reward -= 1\n",
    "        elif action == 5:  # Drop off\n",
    "            if tuple(self.robot_pos) in self.delivery_points:\n",
    "                if 1 in self.inventory:\n",
    "                    package_index = list(self.inventory).index(1)\n",
    "                    self.inventory[package_index] = 0\n",
    "                    reward += 100\n",
    "                    # Check if all packages have been delivered\n",
    "                    if np.sum(self.inventory) == 0:\n",
    "                        done = True  # Episode ends if all packages are delivered\n",
    "                else:\n",
    "                    reward -= 10\n",
    "            else:\n",
    "                reward -= 10\n",
    "\n",
    "        # Check if new position is within grid boundaries\n",
    "        if 0 <= new_pos[0] < self.grid_size and 0 <= new_pos[1] < self.grid_size:\n",
    "            # Check for collision with obstacles\n",
    "            if self.obstacles[new_pos[0]][new_pos[1]] == 1:\n",
    "                reward -= 5  # Negative reward for collision with obstacles\n",
    "                done = True  # Episode ends if collision occurs\n",
    "            else:\n",
    "                # Update robot's position\n",
    "                self.robot_pos = new_pos\n",
    "\n",
    "        # Positive reward for reaching intermediate steps (packages or delivery points)\n",
    "        if reward == 0:\n",
    "            reward += 1\n",
    "\n",
    "        observation = self._get_observation()\n",
    "        return observation, reward, done, {}\n",
    "\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Returns the current observation of the environment.\n",
    "\n",
    "        Returns:\n",
    "        - observation (tuple): Current observation of the environment.\n",
    "        \"\"\"\n",
    "        package_locations = [int(x) for row in self.packages for x in row]\n",
    "        delivery_locations = [int(x) for row in self.delivery_points for x in row]\n",
    "        obstacles = [int(x) for row in self.obstacles for x in row]\n",
    "        inventory = [int(x) for x in self.inventory]\n",
    "        return tuple(self.robot_pos + package_locations + delivery_locations + obstacles + inventory)\n",
    "\n",
    "def hash_state(env, state):\n",
    "    \"\"\"\n",
    "    Hashes the state tuple into a single integer for indexing.\n",
    "\n",
    "    Parameters:\n",
    "    - env (WarehouseEnv): Instance of the Warehouse environment.\n",
    "    - state (tuple): State tuple to be hashed.\n",
    "\n",
    "    Returns:\n",
    "    - hash_value (int): Hashed value of the state.\n",
    "    \"\"\"\n",
    "    hash_value = 0\n",
    "    for i, s in enumerate(state):\n",
    "        hash_value += s * (2 ** i)\n",
    "    return hash_value % env.q_table.shape[0]  # Ensure the hash value is within Q-table bounds\n",
    "\n",
    "\n",
    "def draw_env(screen, env):\n",
    "    \"\"\"\n",
    "    Draws the current state of the environment on the screen.\n",
    "\n",
    "    Parameters:\n",
    "    - screen (pygame.Surface): Pygame surface representing the screen.\n",
    "    - env (WarehouseEnv): Instance of the Warehouse environment.\n",
    "    \"\"\"\n",
    "    screen.fill((255, 255, 255))\n",
    "    cell_size = 50\n",
    "    grid_size = env.grid_size\n",
    "    robot_img = pygame.image.load(\"robot.png\").convert_alpha()\n",
    "    package_img = pygame.image.load(\"package.png\").convert_alpha()\n",
    "    delivery_img = pygame.image.load(\"delivery.png\").convert_alpha()\n",
    "    obstacle_img = pygame.image.load(\"warning.png\").convert_alpha()  # Load obstacle image\n",
    "\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            pygame.draw.rect(screen, (255, 255, 255), (j * cell_size, i * cell_size, cell_size, cell_size))\n",
    "            pygame.draw.rect(screen, (0, 0, 0), (j * cell_size, i * cell_size, cell_size, cell_size), 1)\n",
    "\n",
    "            if env.packages[i][j] == 1:\n",
    "                screen.blit(package_img, (j * cell_size, i * cell_size))\n",
    "            if env.delivery_points[i][j] == 1:\n",
    "                screen.blit(delivery_img, (j * cell_size, i * cell_size))\n",
    "            if env.obstacles[i][j] == 1:  # Draw obstacle if present\n",
    "                screen.blit(obstacle_img, (j * cell_size, i * cell_size))\n",
    "\n",
    "    robot_pos = env.robot_pos\n",
    "    screen.blit(robot_img, (robot_pos[1] * cell_size, robot_pos[0] * cell_size))\n",
    "\n",
    "    pygame.display.flip()\n",
    "\n",
    "\n",
    "def main():\n",
    "    env = WarehouseEnv(grid_size=5, num_packages=3, num_delivery_points=3)\n",
    "\n",
    "    # Print the number of delivery points and packages\n",
    "    print(f\"Number of delivery points: {env.num_delivery_points}\")\n",
    "    print(f\"Number of packages: {env.num_packages}\")\n",
    "\n",
    "    pygame.init()\n",
    "    screen = pygame.display.set_mode((env.grid_size * 50, env.grid_size * 50))\n",
    "    clock = pygame.time.Clock()\n",
    "\n",
    "    num_episodes = 1000\n",
    "    max_steps_per_episode = 1000  # Define maximum steps per episode\n",
    "    alpha = 0.5  # Learning rate\n",
    "    gamma = 0.5  # Discount factor\n",
    "    epsilon = 0.9  # Initial epsilon value\n",
    "    min_epsilon = 0.01  # Minimum epsilon value\n",
    "    decay_rate = 0.9999  # Epsilon decay rate\n",
    "\n",
    "    all_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        print(f\"Current Episode: {episode + 1}\")  # Print current episode\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps = 0  # Initialize step count\n",
    "        exit_flag = False  # Flag to exit event handling loop\n",
    "\n",
    "        # Update epsilon based on decay policy\n",
    "        if epsilon > min_epsilon:\n",
    "            epsilon *= decay_rate\n",
    "            epsilon = max(epsilon, min_epsilon)\n",
    "\n",
    "        while not done and steps < max_steps_per_episode:  # Check maximum steps condition\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    exit_flag = True  # Set the exit flag to True\n",
    "                    break  # Exit the event handling loop\n",
    "            if exit_flag:\n",
    "                break  # Exit the episode loop if the exit flag is True\n",
    "            draw_env(screen, env)\n",
    "\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()  # Choose a random action\n",
    "            else:\n",
    "                # Choose the greedy action based on Q-values\n",
    "                hashed_state = hash_state(env, env._get_observation())\n",
    "                action = np.argmax(env.q_table[hashed_state])\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Update Q-table using Q-learning equation with specified alpha and gamma\n",
    "            hashed_state = hash_state(env, state)\n",
    "            next_hashed_state = hash_state(env, next_state)\n",
    "            best_next_action = np.argmax(env.q_table[next_hashed_state])\n",
    "            env.q_table[hashed_state][action] += alpha * (\n",
    "                reward + gamma * env.q_table[next_hashed_state][best_next_action] - env.q_table[hashed_state][action]\n",
    "            )\n",
    "\n",
    "            total_reward += reward\n",
    "            clock.tick(10)\n",
    "            steps += 1  # Increment step count\n",
    "        print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "        all_rewards.append(total_reward)\n",
    "\n",
    "    pygame.quit()\n",
    "\n",
    "    # Print the Q matrix after training\n",
    "    print(\"Q matrix after training:\")\n",
    "    print(env.q_table)\n",
    "\n",
    "    # Calculate average rewards for each episode\n",
    "    avg_rewards = np.mean(all_rewards, axis=0)\n",
    "\n",
    "    # Plotting\n",
    "    episodes = np.arange(1, num_episodes + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(episodes, avg_rewards)\n",
    "    plt.title(\"Average Cumulative Reward per Episode\")\n",
    "    plt.xlabel(\"Episode Number\")\n",
    "    plt.ylabel(\"Average Cumulative Reward\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c319ccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace45bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with obstacles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399be1fc",
   "metadata": {},
   "source": [
    "# TASK 1 TO 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a8cc06",
   "metadata": {},
   "source": [
    "# IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784f475a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class WarehouseEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Environment representing a warehouse where a robot navigates to pick up packages and deliver them to designated points.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, grid_size, num_packages, num_delivery_points):\n",
    "        \"\"\"\n",
    "        Initializes the Warehouse environment.\n",
    "\n",
    "        Parameters:\n",
    "        - grid_size (int): Size of the grid layout.\n",
    "        - num_packages (int): Number of packages in the warehouse.\n",
    "        - num_delivery_points (int): Number of delivery points in the warehouse.\n",
    "        \"\"\"\n",
    "        super(WarehouseEnv, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.num_packages = num_packages\n",
    "        self.num_delivery_points = num_delivery_points\n",
    "        self.action_space = spaces.Discrete(6)  # Up, Down, Left, Right, Pick up, Drop off\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(grid_size),  # Robot X position\n",
    "            spaces.Discrete(grid_size),  # Robot Y position\n",
    "            spaces.MultiBinary(num_packages),  # Package locations\n",
    "            spaces.MultiBinary(num_delivery_points),  # Delivery locations\n",
    "            spaces.MultiBinary(num_packages)  # Inventory\n",
    "        ))\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment to its initial state.\n",
    "\n",
    "        Returns:\n",
    "        - observation (tuple): Initial observation of the environment.\n",
    "        \"\"\"\n",
    "        self.robot_pos = [0, 0]\n",
    "        self.packages = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.delivery_points = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.obstacles = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.inventory = np.zeros(self.num_packages)\n",
    "        self.package_to_delivery = {}  # Dictionary to store package to delivery point associations\n",
    "\n",
    "        # Randomly place delivery points\n",
    "        for _ in range(self.num_delivery_points):\n",
    "            rand_x = np.random.randint(self.grid_size)\n",
    "            rand_y = np.random.randint(self.grid_size)\n",
    "            while self.delivery_points[rand_x][rand_y] == 1:  # Ensure unique location for each delivery point\n",
    "                rand_x = np.random.randint(self.grid_size)\n",
    "                rand_y = np.random.randint(self.grid_size)\n",
    "            self.delivery_points[rand_x][rand_y] = 1\n",
    "\n",
    "        # Assign packages to remaining empty locations\n",
    "        placed_packages = 0\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                if placed_packages < self.num_packages and self.delivery_points[i][j] == 0:\n",
    "                    self.packages[i][j] = 1\n",
    "                    # Find the nearest delivery point and associate the package with it\n",
    "                    dist_to_delivery = np.inf\n",
    "                    nearest_delivery = None\n",
    "                    for di in range(self.grid_size):\n",
    "                        for dj in range(self.grid_size):\n",
    "                            if self.delivery_points[di][dj] == 1:\n",
    "                                dist = abs(i - di) + abs(j - dj)\n",
    "                                if dist < dist_to_delivery:\n",
    "                                    dist_to_delivery = dist\n",
    "                                    nearest_delivery = (di, dj)\n",
    "                    self.package_to_delivery[(i, j)] = nearest_delivery\n",
    "                    placed_packages += 1\n",
    "\n",
    "        # Randomly place obstacles\n",
    "        num_obstacles = 0\n",
    "        while num_obstacles < 2:  # Add 2 obstacles per episode\n",
    "            rand_x = np.random.randint(self.grid_size)\n",
    "            rand_y = np.random.randint(self.grid_size)\n",
    "            if (\n",
    "                self.packages[rand_x][rand_y] == 0\n",
    "                and self.delivery_points[rand_x][rand_y] == 0\n",
    "                and self.obstacles[rand_x][rand_y] == 0\n",
    "            ):\n",
    "                self.obstacles[rand_x][rand_y] = 1\n",
    "                num_obstacles += 1\n",
    "\n",
    "        self.q_table = np.zeros((2 ** (self.grid_size * 2 + self.num_packages * 2), self.action_space.n))\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Executes one time step in the environment.\n",
    "\n",
    "        Parameters:\n",
    "        - action (int): Action to be taken by the agent.\n",
    "\n",
    "        Returns:\n",
    "        - observation (tuple): New observation of the environment.\n",
    "        - reward (float): Reward received from the environment.\n",
    "        - done (bool): Whether the episode is done or not.\n",
    "        - info (dict): Additional information about the environment.\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        done = False\n",
    "        new_pos = self.robot_pos[:]\n",
    "\n",
    "        if action == 0:  # Move Up\n",
    "            new_pos[0] -= 1\n",
    "        elif action == 1:  # Move Down\n",
    "            new_pos[0] += 1\n",
    "        elif action == 2:  # Move Left\n",
    "            new_pos[1] -= 1\n",
    "        elif action == 3:  # Move Right\n",
    "            new_pos[1] += 1\n",
    "        elif action == 4:  # Pick up\n",
    "            if tuple(self.robot_pos) in self.packages:\n",
    "                package_index = list(self.robot_pos).index(1)\n",
    "                self.inventory[package_index] = 1\n",
    "                self.packages[self.robot_pos[0]][self.robot_pos[1]] = 0\n",
    "                reward += 10  # Add a positive reward for picking up a package\n",
    "            else:\n",
    "                reward -= 1\n",
    "        elif action == 5:  # Drop off\n",
    "            if tuple(self.robot_pos) in self.delivery_points:\n",
    "                if 1 in self.inventory:\n",
    "                    package_index = list(self.inventory).index(1)\n",
    "                    self.inventory[package_index] = 0\n",
    "                    reward += 100\n",
    "                    # Check if all packages have been delivered\n",
    "                    if np.sum(self.inventory) == 0:\n",
    "                        done = True  # Episode ends if all packages are delivered\n",
    "                else:\n",
    "                    reward -= 10\n",
    "            else:\n",
    "                reward -= 10\n",
    "\n",
    "        # Check if new position is within grid boundaries\n",
    "        if 0 <= new_pos[0] < self.grid_size and 0 <= new_pos[1] < self.grid_size:\n",
    "            # Check for collision with obstacles\n",
    "            if self.obstacles[new_pos[0]][new_pos[1]] == 1:\n",
    "                reward -= 5  # Negative reward for collision with obstacles\n",
    "                done = True  # Episode ends if collision occurs\n",
    "            else:\n",
    "                # Update robot's position\n",
    "                self.robot_pos = new_pos\n",
    "\n",
    "        # Positive reward for reaching intermediate steps (packages or delivery points)\n",
    "        if reward == 0:\n",
    "            reward += 1\n",
    "\n",
    "        observation = self._get_observation()\n",
    "        return observation, reward, done, {}\n",
    "\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Returns the current observation of the environment.\n",
    "\n",
    "        Returns:\n",
    "        - observation (tuple): Current observation of the environment.\n",
    "        \"\"\"\n",
    "        package_locations = [int(x) for row in self.packages for x in row]\n",
    "        delivery_locations = [int(x) for row in self.delivery_points for x in row]\n",
    "        obstacles = [int(x) for row in self.obstacles for x in row]\n",
    "        inventory = [int(x) for x in self.inventory]\n",
    "        return tuple(self.robot_pos + package_locations + delivery_locations + obstacles + inventory)\n",
    "\n",
    "def hash_state(env, state):\n",
    "    \"\"\"\n",
    "    Hashes the state tuple into a single integer for indexing.\n",
    "\n",
    "    Parameters:\n",
    "    - env (WarehouseEnv): Instance of the Warehouse environment.\n",
    "    - state (tuple): State tuple to be hashed.\n",
    "\n",
    "    Returns:\n",
    "    - hash_value (int): Hashed value of the state.\n",
    "    \"\"\"\n",
    "    hash_value = 0\n",
    "    for i, s in enumerate(state):\n",
    "        hash_value += s * (2 ** i)\n",
    "    return hash_value % env.q_table.shape[0]  # Ensure the hash value is within Q-table bounds\n",
    "\n",
    "\n",
    "def draw_env(screen, env):\n",
    "    \"\"\"\n",
    "    Draws the current state of the environment on the screen.\n",
    "\n",
    "    Parameters:\n",
    "    - screen (pygame.Surface): Pygame surface representing the screen.\n",
    "    - env (WarehouseEnv): Instance of the Warehouse environment.\n",
    "    \"\"\"\n",
    "    screen.fill((255, 255, 255))\n",
    "    cell_size = 50\n",
    "    grid_size = env.grid_size\n",
    "    robot_img = pygame.image.load(\"robot.png\").convert_alpha()\n",
    "    package_img = pygame.image.load(\"package.png\").convert_alpha()\n",
    "    delivery_img = pygame.image.load(\"delivery.png\").convert_alpha()\n",
    "    obstacle_img = pygame.image.load(\"warning.png\").convert_alpha()  # Load obstacle image\n",
    "\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            pygame.draw.rect(screen, (255, 255, 255), (j * cell_size, i * cell_size, cell_size, cell_size))\n",
    "            pygame.draw.rect(screen, (0, 0, 0), (j * cell_size, i * cell_size, cell_size, cell_size), 1)\n",
    "\n",
    "            if env.packages[i][j] == 1:\n",
    "                screen.blit(package_img, (j * cell_size, i * cell_size))\n",
    "            if env.delivery_points[i][j] == 1:\n",
    "                screen.blit(delivery_img, (j * cell_size, i * cell_size))\n",
    "            if env.obstacles[i][j] == 1:  # Draw obstacle if present\n",
    "                screen.blit(obstacle_img, (j * cell_size, i * cell_size))\n",
    "\n",
    "    robot_pos = env.robot_pos\n",
    "    screen.blit(robot_img, (robot_pos[1] * cell_size, robot_pos[0] * cell_size))\n",
    "\n",
    "    pygame.display.flip()\n",
    "    \n",
    "def main():\n",
    "    env = WarehouseEnv(grid_size=5, num_packages=3, num_delivery_points=3)\n",
    "\n",
    "    # Print the number of delivery points and packages\n",
    "    print(f\"Number of delivery points: {env.num_delivery_points}\")\n",
    "    print(f\"Number of packages: {env.num_packages}\")\n",
    "\n",
    "    pygame.init()\n",
    "    screen = pygame.display.set_mode((env.grid_size * 50, env.grid_size * 50))\n",
    "    clock = pygame.time.Clock()\n",
    "\n",
    "    num_episodes = 1000\n",
    "    max_steps_per_episode = 1000  # Define maximum steps per episode\n",
    "    alpha = 0.5  # Learning rate\n",
    "    gamma = 0.5  # Discount factor\n",
    "    epsilon = 0.9  # Initial epsilon value\n",
    "    min_epsilon = 0.01  # Minimum epsilon value\n",
    "    decay_rate = 0.9999  # Epsilon decay rate\n",
    "\n",
    "    all_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        print(f\"Current Episode: {episode + 1}\")  # Print current episode\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps = 0  # Initialize step count\n",
    "\n",
    "        while not done and steps < max_steps_per_episode:  # Check maximum steps condition\n",
    "            draw_env(screen, env)\n",
    "\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()  # Choose a random action\n",
    "            else:\n",
    "                # Choose the greedy action based on Q-values\n",
    "                hashed_state = hash_state(env, env._get_observation())\n",
    "                action = np.argmax(env.q_table[hashed_state])\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Update Q-table using Q-learning equation with specified alpha and gamma\n",
    "            hashed_state = hash_state(env, state)\n",
    "            next_hashed_state = hash_state(env, next_state)\n",
    "            best_next_action = np.argmax(env.q_table[next_hashed_state])\n",
    "            env.q_table[hashed_state][action] += alpha * (\n",
    "                reward + gamma * env.q_table[next_hashed_state][best_next_action] - env.q_table[hashed_state][action]\n",
    "            )\n",
    "\n",
    "            total_reward += reward\n",
    "            clock.tick(10)\n",
    "            steps += 1  # Increment step count\n",
    "            \n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "            \n",
    "        print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "        all_rewards.append(total_reward)\n",
    "\n",
    "    pygame.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7775f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Provided cumulative rewards data\n",
    "cumulative_rewards = [\n",
    "    -11, -1, -68, -90, -29, -12, -76, -83, -17, -57, -156, -23, -3, -20, -53, -5, -73, -6, -103, -53,\n",
    "    -117, -79, 0, -272, -58, -10, -137, -14, -12, -2, -13, -21, -15, -76, -74, -273, -35, 0, -6, -42,\n",
    "    -5, -268, -11, -12, -23, -24, -50, -5, -13, -243, -51, -3, -85, -14, -59, -61, -227, -316, -69,\n",
    "    -51, -12, -43, -76, -12, -130, -127, 1, -44, -19, -8, -70, -119, -36, -16, -1, -29, -180, -25,\n",
    "    -602, -27, -64, -36, -50, -108, -77, -38, -3, -165, -99, -65, -50, -13, -32, -22, -338, -20,\n",
    "    -24, -57, -5, 3, -81, -95, -9, -48, -114, -185, -101, -114, -36, -69, -21, -39, -20, -11, -117,\n",
    "    -5, -13, -96, -75, 8, -61, -53, -16, -62, -19, -280, -49, -64, -227, -192, 1, -98, -13, -31,\n",
    "    -81, -226, -11, -98, -226, -79, -108, 17, -5, 15, -94, -2, -86, -9, -111, -382,-52, -26, -22, 3, -5, -47, -120, -183, -27, -38, -1, -623, -80, -56, -183, -70, -2, -94, -53,\n",
    "    -6, -26, -77, -2, -367, -154, -35, -177, -73, -29, -41, -102, -25, -62, -20, -113, -42, -159,\n",
    "    -27, -60, -136, -11, 3, -5, -26, -186, -36, -3, -101, -157, -18, -146, -32, -17, -17, -48,\n",
    "    -122, -4, -80, -158, -19, -39, -10, -40, 9, -4, -160, -8, -92, -11, -65, -305, -868, -17,\n",
    "    -11, 1, -156, -165, -12, -7, -34, -2, -42, -15, -74, -29, -104, -30, 1, -12, -22, -6, -11,\n",
    "    -88, -88, -2, -35, -34, -3, -89, -13, -19, -67, -38, -8, -37, -392, -349, -90, -112, -36,\n",
    "    -9, -317, -63, 2, -5, 12, -28, -367, -211, -63, -28, -14, -3, -4, -240, -38, -10, -25, -153,\n",
    "    -15, -31, -162, -30, -8, -75, -44, -20, -44, -22, -19, -683, -27, -92, -50, -54, -159, -35,\n",
    "    -15, -15, -88,-4, -171, -225, -28, -108, -1, -167, -32, -181, -364, -49, -270, -176, -56, -48, -81, -78, -12, -110,\n",
    "    -84, -893, -21, -29, -8, -85, -152, -19, -56, -28, -1, -54, -14, -17, -36, -15, -13, -13, -7, -55, -56,\n",
    "    -86, -152, 11, -4, -76, -4, -32, -7, -15, -20, -82, -58, -10, -349, -4, -18, -1, -223, -76, -266, -15,\n",
    "    -98, -31, -12, -149, -83, -39, -19, -82, -4, -64, -28, -472, -4, -8, -14, -77, -49, -4, -114, -22, -59,\n",
    "    -98, -24, -5, 1, -3, -69, -6, -716, -63, -406, -10, -18, -31, -101, -18, -21, -43, -139, -185, -15, -2,\n",
    "    -18, -8, -166, -66, -158, -211, -95, -155, -3, -392, -61, -16, -28, -97, -60, -23, -120, -4, -97, -23,\n",
    "    -47, -15, -7, 4, 1, -97, -43, -23, -10, -115, -38, -58, -14, -229, -26, -273, -210, -72, -36, -157, -96,\n",
    "    -79, 0, -14, -15, -165, -47, -52, -6, -45, -17, -112, -56, -1, -149, -51, -23, 7, -38, -68, -4, -43,\n",
    "    -178, 1, -48, -16, -2, -3, -58, -68, -10, 1, -12, -47, -63, 6, -3, -176, -129, -3, -16, -80, -24,\n",
    "    -57, -40, -22, -141, 4, -102, -33, -175, -15, -122, -1, -24, -5, -13,-66, 8, -31, -33, -236, -300, -4, \n",
    "    -230, -22, -2, -1, -133, -56, -59, -157, -8, -4, -14, -5, -88, -3, -133, -93, -10, -614, -164, -11, \n",
    "    -234, -5, -84, 1, -9, -61, -102, -49, -19, -46, -14, -195, -48, -62, -132, -53, -3, -45, -34, -24, \n",
    "    -28, -3, -18, -41, -106, -74, -43, -108, -48, -4, -108, -9, -293, -121, -31, 12, -99, -73, -37, 5, 0, \n",
    "    -63, -9, -96, -24, -132, -50, -13, -25, -86, -12, -18, -13, -28, -83, -14, -52, -54, -225, -119, -4, \n",
    "    -29, -54, -14, -5, -20, -57, -10, -282, -20, -11, -64, -71, -35, -16, 5, -14, -14, -21, 5, -83, -57, \n",
    "    -14, -61, -94, -12, -61, -14, -49, -133, -5, -30, -24, -72, -36, -70, -42, -30, -22, -1, -33, -191, \n",
    "    -180, -62, -149, -37, -20, -31, -96, -58, -7, -38, -42, -2, -4, -16, -26, -271, -340, -20, 5, -40, -52, \n",
    "    -287, -7, -322, -4, -58, -51, -43, -36, -29, -57, -11, -17, -489, -10, -153, -7, -14, -68, -19, -105, \n",
    "    -72, -143, -60, -41, -44, -5, -5, -29, -49, -32, -83, -7, -6, -749, -68, -18, -68, -117, -52, -11, -39, \n",
    "    -8, -82, -38, -5, -143, -74, -158, -67, -14, -157, -1, -2, -8, -112, -47, -115, -113, -149, -153, -61,\n",
    "    -67, -128, -142, -39, -20, -2, -4, -292, -24, -37, -96, -17, -39, -3, -121, -123, -430, -9, -12, -264, \n",
    "    -58, -20, -93, -25, -142, -174, -49, -45, -2, -384, -79, -113, -27, -1, -72, -43, -44, -123, -84,-21, 0, -78, -13, 5, -117, -83, -51, -6, -131, -217, -123, -10, -12, -301, \n",
    "    -113, -631, -379, -11, -118, -23, -297, -171, -240, -22, -323, -44, -61, -68, \n",
    "    5, -14, -34, -17, -5, -5, -96, -15, -11, -31, 0, -73, -648, -855, -128, -42, \n",
    "    -78, -127, -33, -3, -90, -28, -73, 6, -204, -3, -21, -111, -99, -5, -42, -85, \n",
    "    -10, 2, -137, -9, -9, -34, -33, -90, -162, -14, -62, -5, -46, -19, -36, -99, \n",
    "    -118, -10, -48, 3, -161, -66, -35, -194, -12, -86, -67, -382, -34, -118, -22, \n",
    "    -25, -29, -19, -41, -76, -99, -48, -74, -35, -159, -81, -308, -120, -18, -34, \n",
    "    -38, -21, -100, -51, -421, -8, -67, -50, 8, -32, -79, -88, -226, -45, -3, -50, \n",
    "    -10, -132, -175, -12, -213, -32, -82, -4, -40, -15, -41, -90, -23, -24, -45, \n",
    "    -53, -121, -28, -90, -5, -97, -54, -229, -2, -83, -181, -35, -5, -541, -17, \n",
    "    -28, -124, -6, -52, -53, -131, -41, -18, -21, -38, -27, -24, -13, -34, -134, \n",
    "    -245, -25, -15, -38, -45, -81, -68, -47, -5, -167, -59, -21, -157, -72, -286, \n",
    "    -43, -70, -415, -34, -127, -13, -68, -20, -30, -36, -22, -275, -271, -246, -32, \n",
    "    -155, -144, -6, -24, -17, -69, -3, -101, -191, -39, -6, -10, 9, -110, -4, -109, \n",
    "    -16, 0, -26, -39, -86, -29, -743, -33, -808, -43, -134, -68, -9, -9, -53, -121, \n",
    "    -22, -7, -66, -29, -2, -96, -111, -47, -5, 4, -287, -5, -34, -230, -95, -15, -319, \n",
    "    -79, -15, -43]\n",
    "\n",
    "\n",
    "# Plotting\n",
    "episodes = range(1, len(cumulative_rewards) + 1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episodes, cumulative_rewards)\n",
    "plt.title(\"Cumulative Reward per Episode\")\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd9a43",
   "metadata": {},
   "source": [
    "# V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96220c5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class WarehouseEnvironment:\n",
    "    def __init__(self, grid_size):\n",
    "        self.grid_size = grid_size\n",
    "        self.state = (0, 0)  # Initial state\n",
    "        self.goal_state = (3, 4)  # Goal state\n",
    "        self.actions = [(1, 0), (0, 1), (-1, 0), (0, -1)]  # Possible actions (down, right, up, left)\n",
    "        self.obstacles = [(2, 1), (3, 3)]  # Obstacle positions\n",
    "        self.package_location = (1, 1)  # Package location\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "      if action in self.actions:\n",
    "          next_state = (self.state[0] + action[0], self.state[1] + action[1])\n",
    "          next_state = (max(0, min(next_state[0], self.grid_size - 1)),\n",
    "                        max(0, min(next_state[1], self.grid_size - 1)))  # Ensure state is within bounds\n",
    "\n",
    "          # Check if the agent is trying to pick up the package\n",
    "          if next_state == self.package_location and self.package_location != (-1, -1):\n",
    "              # Agent picks up the package\n",
    "              self.package_location = (-1, -1)  # Package is picked up\n",
    "          elif next_state == self.goal_state and self.package_location == (-1, -1):\n",
    "              # Agent drops off the package\n",
    "              self.package_location = self.goal_state  # Package is dropped off\n",
    "          elif next_state == self.goal_state and self.package_location != (-1, -1):\n",
    "              # Agent tries to drop off the package in the wrong location\n",
    "              reward = -10\n",
    "              return self.state, reward, False  # Return the current state with the penalty and mark episode as not done\n",
    "\n",
    "          if next_state not in self.obstacles:\n",
    "              self.state = next_state\n",
    "          else:\n",
    "              reward = -5  # Apply penalty if the agent crosses an obstacle\n",
    "      else:\n",
    "          reward = -5  # Apply penalty if the action is invalid\n",
    "\n",
    "      done = self.state == self.goal_state and self.package_location == self.goal_state\n",
    "      reward = 50 if done else (reward if 'reward' in locals() else -1)  # Reward is 50 for reaching the goal with the package, -1 otherwise\n",
    "\n",
    "      return self.state, reward, done\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def visualize_grid(self):\n",
    "        grid = [['-' for _ in range(self.grid_size)] for _ in range(self.grid_size)]\n",
    "        grid[self.state[0]][self.state[1]] = 'A'  # Agent\n",
    "        grid[self.goal_state[0]][self.goal_state[1]] = 'G'  # Goal\n",
    "\n",
    "        grid[self.package_location[0]][self.package_location[1]] = 'P'  # Package\n",
    "\n",
    "        for obstacle in self.obstacles:\n",
    "            grid[obstacle[0]][obstacle[1]] = 'X'  # Obstacle\n",
    "\n",
    "        for row in grid:\n",
    "            print(' '.join(row))\n",
    "        print()\n",
    "\n",
    "\n",
    "def q_learning(env, num_episodes, alpha, gamma, epsilon):\n",
    "    q_table = np.zeros((env.grid_size, env.grid_size, len(env.actions)))\n",
    "    total_rewards = []\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if np.random.uniform(0, 1) < epsilon:  # Epsilon-greedy policy\n",
    "                action = np.random.randint(0, len(env.actions))\n",
    "            else:\n",
    "                action = np.argmax(q_table[state[0], state[1]])\n",
    "\n",
    "            next_state, reward, done = env.step(env.actions[action])\n",
    "            total_reward += reward\n",
    "            q_table[state[0], state[1], action] += alpha * (reward + gamma * np.max(q_table[next_state[0], next_state[1]]) - q_table[state[0], state[1], action])\n",
    "            state = next_state\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "    return total_rewards\n",
    "\n",
    "# Define environment parameters\n",
    "grid_size = 5\n",
    "num_episodes = 1000\n",
    "alphas = [0.9, 0.1, 0.5]  # Different alpha values to try\n",
    "gammas = [0.9, 0.5, 0.2]  # Different gamma values to try\n",
    "epsilon = 0.2  # Epsilon for epsilon-greedy policy\n",
    "\n",
    "# Print initial state of the grid before trying different combinations\n",
    "env = WarehouseEnvironment(grid_size)\n",
    "env.visualize_grid()\n",
    "\n",
    "# Train Q-learning agent for different combinations of alpha and gamma\n",
    "for alpha in alphas:\n",
    "    for gamma in gammas:\n",
    "        total_rewards = q_learning(env, num_episodes, alpha, gamma, epsilon)\n",
    "\n",
    "        # Calculate average reward for each episode\n",
    "        average_rewards = [np.mean(total_rewards[:i+1]) for i in range(num_episodes)]\n",
    "\n",
    "        # Plot\n",
    "        plt.plot(range(1, num_episodes+1), average_rewards, label=f'alpha={alpha}, gamma={gamma}')\n",
    "\n",
    "plt.xlabel('Episode Number')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward per Episode')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ea49ff",
   "metadata": {},
   "source": [
    "# TASK 7 AND 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eef282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Motivation: Target Networks: Target Networks are used to stabilize training in Deep Q-Learning. By maintaining a separate target network with fixed parameters for estimating Q-values, we reduce the risk of divergence during training. This improvement helps in achieving more stable and reliable learning.\n",
    "#Double DQN: Double DQN addresses the overestimation bias in traditional Q-learning algorithms. By decoupling the action selection and evaluation steps, Double DQN leads to more accurate Q-value estimations, resulting in improved learning performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd55bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41965afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pygame\n",
    "import time\n",
    "import gym\n",
    "import random\n",
    "from keras import Sequential\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.activations import relu, linear\n",
    "\n",
    "\n",
    "class DeepQNetwork:\n",
    "\n",
    "\n",
    "    def __init__(self, action_space, state_space, learning_rate=0.8):\n",
    "\n",
    "        self.epsilon = 1.0\n",
    "        self.gamma = .95\n",
    "        self.batch_size = 64\n",
    "        self.epsilon_min = .01\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon_decay = .90\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.action_space_size = action_space\n",
    "        self.state_space_shape = state_space\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(20, input_dim=self.state_space_shape, activation=relu))\n",
    "        model.add(Dense(25, activation=relu))\n",
    "        model.add(Dense(self.action_space_size, activation=linear))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def rememberFunction(self, state, action, reward, nextState, done):\n",
    "        self.memory.append((state, action, reward, nextState, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_space_size)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        actValues = self.model.predict(state)\n",
    "        return np.argmax(actValues[0])\n",
    "\n",
    "    def replayFunction(self):\n",
    "\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        miniBatchVar = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([i[0] for i in miniBatchVar])\n",
    "        actions = np.array([i[1] for i in miniBatchVar])\n",
    "        rewards = np.array([i[2] for i in miniBatchVar])\n",
    "        nextStates = np.array([i[3] for i in miniBatchVar])\n",
    "        dones = np.array([i[4] for i in miniBatchVar])\n",
    "\n",
    "        states = np.squeeze(states)\n",
    "        nextStates = np.squeeze(nextStates)\n",
    "\n",
    "        targets = rewards + self.gamma * (np.amax(self.model.predict_on_batch(nextStates), axis=1)) * (1 - dones)\n",
    "        targetsFull = self.model.predict_on_batch(states)\n",
    "\n",
    "        indexes = np.array([i for i in range(self.batch_size)])\n",
    "        targetsFull[[indexes], [actions]] = targets\n",
    "\n",
    "        self.model.fit(states, targetsFull, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "def rewardFunction(nextStateInfo):\n",
    "    nextState = nextStateInfo[0]  # Extracting the nextState array\n",
    "    if nextState[0] >= 0.5:\n",
    "        print(\"Reached Goal\")\n",
    "        return 10\n",
    "    if nextState[0] > -0.4:\n",
    "        return (1 + nextState[0]) ** 2\n",
    "    return 0\n",
    "\n",
    "\n",
    "def trainDQNetwork(environment, agent, episode):\n",
    "    episodeScores = []\n",
    "    for e in range(episode):\n",
    "        state = environment.reset()[0]  # Extracting the state array\n",
    "        score = 0\n",
    "        maxSteps = 1000\n",
    "        for i in range(maxSteps):\n",
    "            environment.render()\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    quit()\n",
    "            state = np.array(state)  # Convert state tuple to numpy array\n",
    "            action = agent.act(state)\n",
    "            stepResult = environment.step(action)  # get all return values\n",
    "            nextState, reward, done = stepResult[:3]  # get the first three elements\n",
    "            reward = rewardFunction(stepResult)  # pass values to rewardFunction function\n",
    "            score += reward\n",
    "            nextState = np.array(nextState)  \n",
    "            agent.rememberFunction(state, action, reward, nextState, done)\n",
    "            state = nextState\n",
    "            agent.replayFunction()\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
    "                break\n",
    "        episodeScores.append(score)\n",
    "    return episodeScores\n",
    "\n",
    "def main():\n",
    "    pygame.init()  # initialize Pygame\n",
    "    environment = gym.make('MountainCar-v0', render_mode=\"human\") # render as human\n",
    "    np.random.seed(10)  # numpy random seed\n",
    "\n",
    "    print(environment.observation_space)\n",
    "    print(environment.action_space)\n",
    "    agent = DeepQNetwork(environment.action_space.n, environment.observation_space.shape[0], learning_rate=0.001)\n",
    "    episodes = 100\n",
    "    episodeScores = trainDQNetwork(environment, agent, episodes)\n",
    "    plt.plot([i+1 for i in range(episodes)], episodeScores)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c41ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2088a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pygame\n",
    "import gym\n",
    "import random\n",
    "from keras import Sequential\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.activations import relu, linear\n",
    "\n",
    "class DoubleDeepQNetwork:\n",
    "    def __init__(self, action_space, state_space, learning_rate=0.001):\n",
    "        self.epsilon = 1.0\n",
    "        self.gamma = 0.99\n",
    "        self.batch_size = 64\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.action_space_size = action_space\n",
    "        self.state_space_shape = state_space\n",
    "        self.model = self.build_model(learning_rate)\n",
    "        self.target_model = self.build_model(learning_rate)\n",
    "\n",
    "    def build_model(self, learning_rate):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(20, input_dim=self.state_space_shape, activation=relu))\n",
    "        model.add(Dense(25, activation=relu))\n",
    "        model.add(Dense(self.action_space_size, activation=linear))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_space_size)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([i[0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "\n",
    "        states = np.squeeze(states)\n",
    "        next_states = np.squeeze(next_states)\n",
    "\n",
    "        target_actions = np.argmax(self.model.predict_on_batch(next_states), axis=1)\n",
    "        target_q_values = self.target_model.predict_on_batch(next_states)\n",
    "        targets = rewards + self.gamma * target_q_values[np.arange(self.batch_size), target_actions] * (1 - dones)\n",
    "\n",
    "        targets_full = self.model.predict_on_batch(states)\n",
    "        targets_full[np.arange(self.batch_size), actions] = targets\n",
    "\n",
    "        self.model.fit(states, targets_full, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        if len(self.memory) % 1000 == 0:\n",
    "            self.update_target_model()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "def get_reward(next_state_info):\n",
    "    next_state = next_state_info[0]\n",
    "    if next_state[0] >= 0.5:\n",
    "        print(\"Car has reached the goal\")\n",
    "        return 10\n",
    "    if next_state[0] > -0.4:\n",
    "        return (1 + next_state[0]) ** 2\n",
    "    return 0\n",
    "\n",
    "def train_ddqn(environment, agent, episode):\n",
    "    episode_scores = []\n",
    "    for e in range(episode):\n",
    "        state = environment.reset()[0]\n",
    "        score = 0\n",
    "        max_steps = 1000\n",
    "        for i in range(max_steps):\n",
    "            state = np.array(state)\n",
    "            action = agent.act(state)\n",
    "            step_result = environment.step(action)\n",
    "            next_state, reward, done = step_result[:3]\n",
    "            reward = get_reward(step_result)\n",
    "            score += reward\n",
    "            next_state = np.array(next_state)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            agent.replay()\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
    "                break\n",
    "        episode_scores.append(score)\n",
    "    return episode_scores\n",
    "\n",
    "def main():\n",
    "    pygame.init()\n",
    "    environment = gym.make('MountainCar-v0', render_mode=\"human\")\n",
    "    np.random.seed(10)\n",
    "\n",
    "    print(environment.observation_space)\n",
    "    print(environment.action_space)\n",
    "    agent = DoubleDeepQNetwork(environment.action_space.n, environment.observation_space.shape[0], learning_rate=0.001)\n",
    "    episodes = 60\n",
    "    episode_scores = train_ddqn(environment, agent, episodes)\n",
    "    plt.plot([i+1 for i in range(episodes)], episode_scores)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bd1145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d1c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pygame\n",
    "import time\n",
    "import gym\n",
    "import random\n",
    "from keras import Sequential\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.activations import relu, linear\n",
    "\n",
    "\n",
    "class DeepQNetwork:\n",
    "    def __init__(self, action_space, state_space, learning_rate=0.001):\n",
    "        self.epsilon = 1.0\n",
    "        self.gamma = 0.95\n",
    "        self.batch_size = 64\n",
    "        self.epsilon_min = 0.01\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.action_space_size = action_space\n",
    "        self.state_space_shape = state_space\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(20, input_dim=self.state_space_shape, activation=relu))\n",
    "        model.add(Dense(25, activation=relu))\n",
    "        model.add(Dense(self.action_space_size, activation=linear))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def rememberFunction(self, state, action, reward, nextState, done):\n",
    "        self.memory.append((state, action, reward, nextState, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_space_size)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        actValues = self.model.predict(state)\n",
    "        return np.argmax(actValues[0])\n",
    "\n",
    "    def replayFunction(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        miniBatchVar = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([i[0] for i in miniBatchVar])\n",
    "        actions = np.array([i[1] for i in miniBatchVar])\n",
    "        rewards = np.array([i[2] for i in miniBatchVar])\n",
    "        nextStates = np.array([i[3] for i in miniBatchVar])\n",
    "        dones = np.array([i[4] for i in miniBatchVar])\n",
    "\n",
    "        states = np.squeeze(states)\n",
    "        nextStates = np.squeeze(nextStates)\n",
    "\n",
    "        targets = rewards + self.gamma * (np.amax(self.target_model.predict_on_batch(nextStates), axis=1)) * (1 - dones)\n",
    "        targetsFull = self.model.predict_on_batch(states)\n",
    "\n",
    "        indexes = np.array([i for i in range(self.batch_size)])\n",
    "        targetsFull[[indexes], [actions]] = targets\n",
    "\n",
    "        self.model.fit(states, targetsFull, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def soft_update_target_network(self, tau):\n",
    "        model_weights = self.model.get_weights()\n",
    "        target_model_weights = self.target_model.get_weights()\n",
    "\n",
    "        new_weights = []\n",
    "        for model_weight, target_model_weight in zip(model_weights, target_model_weights):\n",
    "            new_weight = tau * model_weight + (1 - tau) * target_model_weight\n",
    "            new_weights.append(new_weight)\n",
    "\n",
    "        self.target_model.set_weights(new_weights)\n",
    "\n",
    "\n",
    "\n",
    "def rewardFunction(nextStateInfo):\n",
    "    nextState = nextStateInfo[0]  # Extracting the nextState array\n",
    "    if nextState[0] >= 0.5:\n",
    "        print(\"Reached Goal\")\n",
    "        return 10\n",
    "    if nextState[0] > -0.4:\n",
    "        return (1 + nextState[0]) ** 2\n",
    "    return 0\n",
    "\n",
    "\n",
    "def trainDQNetwork(environment, agent, episode):\n",
    "    episodeScores = []\n",
    "    for e in range(episode):\n",
    "        state = environment.reset()[0]  # Extracting the state array\n",
    "        score = 0\n",
    "        maxSteps = 1000\n",
    "        for i in range(maxSteps):\n",
    "            environment.render()\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    quit()\n",
    "            state = np.array(state)  # Convert state tuple to numpy array\n",
    "            action = agent.act(state)\n",
    "            stepResult = environment.step(action)  # get all return values\n",
    "            nextState, reward, done = stepResult[:3]  # get the first three elements\n",
    "            reward = rewardFunction(stepResult)  # pass values to rewardFunction function\n",
    "            score += reward\n",
    "            nextState = np.array(nextState)\n",
    "            agent.rememberFunction(state, action, reward, nextState, done)\n",
    "            state = nextState\n",
    "            agent.replayFunction()\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, episode, score))\n",
    "                break\n",
    "        episodeScores.append(score)\n",
    "        agent.soft_update_target_network(0.01)  # Soft update the target network\n",
    "    return episodeScores\n",
    "\n",
    "\n",
    "def main():\n",
    "    pygame.init()  # initialize Pygame\n",
    "    environment = gym.make('MountainCar-v0', render_mode=\"human\")  # render as human\n",
    "    np.random.seed(10)  # numpy random seed\n",
    "\n",
    "    print(environment.observation_space)\n",
    "    print(environment.action_space)\n",
    "    agent = DeepQNetwork(environment.action_space.n, environment.observation_space.shape[0], learning_rate=0.001)\n",
    "    episodes = 100\n",
    "    episodeScores = trainDQNetwork(environment, agent, episodes)\n",
    "    plt.plot([i + 1 for i in range(episodes)], episodeScores)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d9cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d017f3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_episodes):\n\u001b[1;32m     82\u001b[0m     state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 83\u001b[0m     state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(state, [\u001b[38;5;241m1\u001b[39m, state_size])\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m time \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m7000\u001b[39m):\n\u001b[1;32m     85\u001b[0m         action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:298\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape\u001b[39m(a, newshape, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m           [5, 6]])\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m'\u001b[39m, newshape, order\u001b[38;5;241m=\u001b[39morder)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     wrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(asarray(obj), method)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, mu\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "env = gym.make('MountainCar-v0', render_mode=\"human\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "batch_size = 32\n",
    "n_episodes = 70000\n",
    "output_dir = 'model/'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.env = env\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=200000)\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.85\n",
    "        self.epsilon_min = 0.00001\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = tf.keras.models.Sequential()\n",
    "        state_shape = self.env.observation_space.shape\n",
    "        model.add(tf.keras.layers.Dense(24, input_shape=state_shape, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(48, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states = []\n",
    "        targets = []\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.target_model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            states.append(state[0])\n",
    "            targets.append(target_f[0])\n",
    "        self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "done = False\n",
    "counter = 0\n",
    "scores_memory = deque(maxlen=100)\n",
    "for e in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(7000):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, halp = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            scores_memory.append(time)\n",
    "            scores_avg = np.mean(scores_memory) * -1\n",
    "            print('episode: {}/{}, score: {}, e {:.2}, help: {}, reward: {}, 100score avg: {}'.format(e, n_episodes, time, agent.epsilon, state, reward, scores_avg))\n",
    "            break\n",
    "    agent.update_target_model()\n",
    "    if agent.epsilon > agent.epsilon_min:\n",
    "        agent.epsilon *= agent.epsilon_decay\n",
    "    if e % 50 == 0:\n",
    "        agent.save(output_dir + 'weights_final' + '{:04d}'.format(e) + \".hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ebcd79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
